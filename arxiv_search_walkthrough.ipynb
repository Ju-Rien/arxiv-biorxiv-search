{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's specify the parameters as though we were going to pass them to the arxivsearch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params\n",
    "start_date = str(datetime.date.today().replace(day=1))\n",
    "end_date = str(datetime.date.today().replace(day=10))\n",
    "kwd_req = ['online'] \n",
    "kwd_exc = ['education'] \n",
    "kwd_one = [['bandit', 'partial information'], ['regret', 'generalization']]\n",
    "athr_req = [] \n",
    "athr_exc = [] \n",
    "athr_one = []\n",
    "subject = 'stat' # subjects = ['stat']\n",
    "max_records = 50\n",
    "max_time = 300\n",
    "cols = ['id', 'title', 'authors', 'date', 'categories', 'abstract']\n",
    "export = '/Users/blairbilodeau/Desktop/arxiv/'\n",
    "exportfile = ''\n",
    "download = '/Users/blairbilodeau/Desktop/arxiv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try accessing the url, which we print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url to extract papers from this subject\n",
    "OAI = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "ARXIV = '{http://arxiv.org/OAI/arXiv/}'\n",
    "BASE = 'http://export.arxiv.org/oai2?verb=ListRecords&'\n",
    "\n",
    "url = BASE + 'from=' + start_date + '&until=' + end_date + '&metadataPrefix=arXiv&set=%s' %subject\n",
    "\n",
    "## error handling for opening and closing URL\n",
    "try:\n",
    "\t#url_response = urlopen(url)\n",
    "\t#xml = url_response.read() # get raw xml data from server\n",
    "\tgc.collect()\n",
    "except HTTPError as e:\n",
    "\t# handle service unavailable error for requesting too often\n",
    "\tif e.code == 503:\n",
    "\t\tretry_time = int(e.hdrs.get('retry-after', 30))\n",
    "\t\tprint('Got 503. Retry after {0:d} seconds.'.format(retry_time))\n",
    "\t# if it's a different error, just have to raise it\n",
    "\telse:\n",
    "\t\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/oai2?verb=ListRecords&from=2020-05-01&until=2020-05-10&metadataPrefix=arXiv&set=stat\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://from=2020-05-01&until=2020-05-10'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://from=' + start_date + '&until=' + end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280168"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xml[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a bit how ugly XML is for humans, and start extracting elements from it we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_root = ET.fromstring(xml) # get root of xml hierarchy\n",
    "print(xml_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = xml_root.findall(OAI + 'ListRecords/' + OAI + 'record') # list of all records from xml tree\n",
    "print('We have fetched {0:d} records. Here is a sample.'.format(len(records)))\n",
    "print(records[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract metadata for each record\n",
    "metadata = [record.find(OAI + 'metadata').find(ARXIV + 'arXiv') for record in records]\n",
    "print(metadata[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Python's list comprehension to extract the data. Here's an example of how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "y=  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "## list comprehension\n",
    "x = []\n",
    "for i in range(10):\n",
    "\tx = x + [i]\n",
    "print('x = ',x)\n",
    "\n",
    "# v.s.\n",
    "\n",
    "y = [i for i in range(10)]\n",
    "print('y = ',y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
